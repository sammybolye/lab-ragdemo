{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3459a160",
   "metadata": {},
   "source": [
    "# 03. Fine-Tuning Primer (PEFT/LoRA)\n",
    "**Warning: CPU Training is Slow**\n",
    "\n",
    "In this notebook, we welcome you to the world of Fine-Tuning!\n",
    "We will use **TinyLlama-1.1B** and **LoRA** (Low-Rank Adaptation) to demonstrate the mechanics.\n",
    "\n",
    "**Steps:**\n",
    "1. Load Model & Tokenizer (TinyLlama).\n",
    "2. Prepare a tiny dataset.\n",
    "3. Configure LoRA.\n",
    "4. Run a Training Loop (1-2 steps only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66cb9bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Use CPU (or mps if available on Mac host, but inside Docker usually CPU)\n",
    "device = 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d4d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyLlama/TinyLlama-1.1B-Chat-v1.0...\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Model (TinyLlama)\n",
    "model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "print(f'Loading {model_id}...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8ed608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 15\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare Dummy Dataset\n",
    "data = [\n",
    "    {'text': 'User: How do I learn RAG? Assistant: Start with the RAG Lab! '},\n",
    "    {'text': 'User: What is Docker? Assistant: A tool to containerize apps. '},\n",
    "    {'text': 'User: Who is Antigravity? Assistant: An agentic AI coding assistant by google. '}\n",
    "] * 5  # Repeat to make it slightly bigger\n",
    "\n",
    "dataset = Dataset.from_list(data)\n",
    "print(f'Dataset size: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b1b109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 563,200 || all params: 1,100,611,584 || trainable%: 0.0512\n"
     ]
    }
   ],
   "source": [
    "# 3. Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=4,            # Rank\n",
    "    lora_alpha=16, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdb97f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "444aeee0b6fd4c58aecb037de017964f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1c4c410e9b4b48888fa374f5b9e158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467b0617e75b47cbaaf2e21d9d82efdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (this might take a minute)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30/30 00:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.642700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>4.599500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>4.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.963600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# 4. Train (1 Step Demo)\n",
    "# Updated for TRL >= 0.25 (Using SFTConfig)\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field='text',\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=500,\n",
    "    per_device_train_batch_size=1,\n",
    "    max_steps=30,  # Increased to 30 steps for better learning (approx 1-2 mins on CPU)\n",
    "    logging_steps=5,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print('Starting training (this might take a minute)...')\n",
    "trainer.train()\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c9b059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter saved to ./tinyllama_lora_adapter\n"
     ]
    }
   ],
   "source": [
    "# 5. Save the Component (Adapter)\n",
    "# We don't save the whole 1GB model, just the tiny difference (LoRA)\n",
    "output_adapter_dir = './tinyllama_lora_adapter'\n",
    "trainer.save_model(output_adapter_dir)\n",
    "print(f'Adapter saved to {output_adapter_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a50acb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading base model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelForCausalLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Inference usually runs in a fresh process, but here we reload\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mReloading base model...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m base_model = \u001b[43mAutoModelForCausalLM\u001b[49m.from_pretrained(model_id)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading your new adapters...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m finetuned_model = PeftModel.from_pretrained(base_model, output_adapter_dir)\n",
      "\u001b[31mNameError\u001b[39m: name 'AutoModelForCausalLM' is not defined"
     ]
    }
   ],
   "source": [
    "# 6. Inference (Try it out!)\n",
    "from peft import PeftModel\n",
    "\n",
    "# Inference usually runs in a fresh process, but here we reload\n",
    "print('Reloading base model...')\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "print('Loading your new adapters...')\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, output_adapter_dir)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = 'User: Who is Antigravity? Assistant: '\n",
    "inputs = tokenizer(test_prompt, return_tensors='pt')\n",
    "\n",
    "print('Generating...')\n",
    "outputs = finetuned_model.generate(**inputs, max_new_tokens=30)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print('-'*20)\n",
    "print(result)\n",
    "print('-'*20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
